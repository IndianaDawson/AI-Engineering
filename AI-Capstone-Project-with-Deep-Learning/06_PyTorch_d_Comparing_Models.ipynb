{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<h1><h1>Pre-trained-Models with PyTorch </h1>"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "In this lab, you will use pre-trained models to classify between the negative and positive samples; you will be provided with the dataset object. The particular pre-trained model will be resnet18; you will have three questions: \n<ul>\n<li>change the output layer</li>\n<li> train the model</li> \n<li>  identify  several  misclassified samples</li> \n </ul>\nYou will take several screenshots of your work and share your notebook. "
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<h2 id=\"download_data\">Download Data</h2>"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Download the dataset and unzip the files in your data directory, unlike the other labs, all the data will be deleted after you close  the lab, this may take some time:"
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "--2020-09-10 03:54:47--  https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/Positive_tensors.zip\nResolving s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)... 67.228.254.196\nConnecting to s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)|67.228.254.196|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 2598656062 (2.4G) [application/zip]\nSaving to: \u2018Positive_tensors.zip\u2019\n\n100%[====================================>] 2,598,656,062 46.0MB/s   in 57s    \n\n2020-09-10 03:55:44 (43.7 MB/s) - \u2018Positive_tensors.zip\u2019 saved [2598656062/2598656062]\n\n"
                }
            ],
            "source": "!wget https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/Positive_tensors.zip "
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Archive:  Positive_tensors.zip\r\n"
                }
            ],
            "source": "!unzip -n Positive_tensors.zip "
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "--2020-09-10 03:57:02--  https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/Negative_tensors.zip\nResolving s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)... 67.228.254.196\nConnecting to s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)|67.228.254.196|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 2111408108 (2.0G) [application/zip]\nSaving to: \u2018Negative_tensors.zip\u2019\n\n100%[====================================>] 2,111,408,108 49.9MB/s   in 45s    \n\n2020-09-10 03:57:47 (45.2 MB/s) - \u2018Negative_tensors.zip\u2019 saved [2111408108/2111408108]\n\n"
                }
            ],
            "source": "!wget https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/Negative_tensors.zip"
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Archive:  Negative_tensors.zip\r\n"
                }
            ],
            "source": "!unzip -n Negative_tensors.zip"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "We will install torchvision:"
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Requirement already satisfied: torch in /opt/conda/envs/Python36/lib/python3.6/site-packages (1.6.0)\nRequirement already satisfied: numpy in /opt/conda/envs/Python36/lib/python3.6/site-packages (from torch) (1.15.4)\nRequirement already satisfied: future in /opt/conda/envs/Python36/lib/python3.6/site-packages (from torch) (0.17.1)\nRequirement already satisfied: torchvision in /opt/conda/envs/Python36/lib/python3.6/site-packages (0.7.0)\nRequirement already satisfied: pillow>=4.1.1 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from torchvision) (5.4.1)\nRequirement already satisfied: numpy in /opt/conda/envs/Python36/lib/python3.6/site-packages (from torchvision) (1.15.4)\nRequirement already satisfied: torch==1.6.0 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from torchvision) (1.6.0)\nRequirement already satisfied: future in /opt/conda/envs/Python36/lib/python3.6/site-packages (from torch==1.6.0->torchvision) (0.17.1)\n"
                }
            ],
            "source": "!pip install torch\n!pip install torchvision"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<h2 id=\"auxiliary\">Imports and Auxiliary Functions</h2>"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "The following are the libraries we are going to use for this lab. The <code>torch.manual_seed()</code> is for forcing the random function to give the same number every time we try to recompile it."
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "<torch._C.Generator at 0x7f05b84d52e8>"
                    },
                    "execution_count": 6,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "# These are the libraries will be used for this lab.\nimport torchvision.models as models\nfrom PIL import Image\nimport pandas\nfrom torchvision import transforms\nimport torch.nn as nn\nimport time\nimport torch \nimport matplotlib.pylab as plt\nimport numpy as np\nfrom torch.utils.data import Dataset, DataLoader\nimport h5py\nimport os\nimport glob\ntorch.manual_seed(0)"
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": "from matplotlib.pyplot import imshow\nimport matplotlib.pylab as plt\nfrom PIL import Image\nimport pandas as pd\nimport os"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<!--Empty Space for separating topics-->"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<h2 id=\"data_class\">Dataset Class</h2>"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": " This dataset class is essentially the same dataset you build in the previous section, but to speed things up, we are going to use tensors instead of jpeg images. Therefor for each iteration, you will skip the reshape step, conversion step to tensors and normalization step."
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "done\n"
                }
            ],
            "source": "# Create your own dataset object\n\nclass Dataset(Dataset):\n\n    # Constructor\n    def __init__(self,transform=None,train=True):\n        #directory=\"/home/dsxuser/work\"\n        positive=\"./Positive_tensors\"\n        negative='./Negative_tensors'\n\n        positive_file_path=os.path.join(positive)\n        negative_file_path=os.path.join(negative)\n        positive_files=[os.path.join(positive_file_path,file) for file in os.listdir(positive_file_path) if file.endswith(\".pt\")]\n        negative_files=[os.path.join(negative_file_path,file) for file in os.listdir(negative_file_path) if file.endswith(\".pt\")]\n        number_of_samples=len(positive_files)+len(negative_files)\n        self.all_files=[None]*number_of_samples\n        self.all_files[::2]=positive_files\n        self.all_files[1::2]=negative_files \n        # The transform is goint to be used on image\n        self.transform = transform\n        #torch.LongTensor\n        self.Y=torch.zeros([number_of_samples]).type(torch.LongTensor)\n        self.Y[::2]=1\n        self.Y[1::2]=0\n        \n        if train:\n            self.all_files=self.all_files[0:30000]\n            self.Y=self.Y[0:30000]\n            self.len=len(self.all_files)\n        else:\n            self.all_files=self.all_files[30000:]\n            self.Y=self.Y[30000:]\n            self.len=len(self.all_files)     \n       \n    # Get the length\n    def __len__(self):\n        return self.len\n    \n    # Getter\n    def __getitem__(self, idx):\n               \n        image=torch.load(self.all_files[idx])\n        y=self.Y[idx]\n                  \n        # If there is any transform method, apply it onto the image\n        if self.transform:\n            image = self.transform(image)\n\n        return image, y\n    \nprint(\"done\")"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "We create two dataset objects, one for the training data and one for the validation data."
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "done\n"
                }
            ],
            "source": "train_dataset = Dataset(train=True)\nvalidation_dataset = Dataset(train=False)\nprint(\"done\")"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<h2 id=\"Question_1\">Question 1</h2>"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<b>Prepare a pre-trained resnet18 model :</b>"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<b>Step 1</b>: Load the pre-trained model <code>resnet18</code> Set the parameter <code>pretrained</code> to true:"
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /home/dsxuser/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth\n"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "ae040fa8a1dc481b8107258a0d24c894",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": "HBox(children=(IntProgress(value=0, max=46827520), HTML(value='')))"
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "\n"
                }
            ],
            "source": "# Step 1: Load the pre-trained model resnet18\n\nmodel = models.resnet18(pretrained=True)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<b>Step 2</b>: Set the attribute <code>requires_grad</code> to <code>False</code>. As a result, the parameters will not be affected by training."
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [],
            "source": "# Step 2: Set the parameter cannot be trained for the pre-trained model\n\nfor param in model.parameters():\n    param.requires_grad = False"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<code>resnet18</code> is used to classify 1000 different objects; as a result, the last layer has 1000 outputs.  The 512 inputs come from the fact that the previously hidden layer has 512 outputs. "
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<b>Step 3</b>: Replace the output layer <code>model.fc</code> of the neural network with a <code>nn.Linear</code> object, to classify 2 different classes. For the parameters <code>in_features </code> remember the last hidden layer has 512 neurons."
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [],
            "source": "model.fc = nn.Linear(512, 2)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Print out the model in order to show whether you get the correct answer.<br> <b>(Your peer reviewer is going to mark based on what you print here.)</b>"
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "ResNet(\n  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (fc): Linear(in_features=512, out_features=2, bias=True)\n)\n"
                }
            ],
            "source": "print(model)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<h2 id=\"Question_2\">Question 2: Train the Model</h2>"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "In this question you will train your, model:"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<b>Step 1</b>: Create a cross entropy criterion function "
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [],
            "source": "# Step 1: Create the loss function\n\ncriterion = nn.CrossEntropyLoss()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<b>Step 2</b>: Create a training loader and validation loader object, the batch size should have 100 samples each."
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [],
            "source": "train_loader      = DataLoader(dataset=train_dataset,      batch_size=100)\nvalidation_loader = DataLoader(dataset=validation_dataset, batch_size=100)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<b>Step 3</b>: Use the following optimizer to minimize the loss "
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [],
            "source": "optimizer = torch.optim.Adam([parameters  for parameters in model.parameters() if parameters.requires_grad],lr=0.001)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<!--Empty Space for separating topics-->"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "**Complete the following code to calculate  the accuracy on the validation data for one epoch; this should take about 45 minutes. Make sure you calculate the accuracy on the validation data.**"
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [],
            "source": "n_epochs=1\nloss_list=[]\naccuracy_list=[]\ncorrect=0\nN_test=len(validation_dataset)\nN_train=len(train_dataset)\nstart_time = time.time()\n#n_epochs\n\nLoss=0\nstart_time = time.time()\nfor epoch in range(n_epochs):\n    for x, y in train_loader:\n\n        model.train() \n        #clear gradient \n        optimizer.zero_grad()\n        #make a prediction \n        z = model(x)\n        # calculate loss \n        loss = criterion(z, y) \n        # calculate gradients of parameters \n        loss.backward()\n        # update parameters \n        optimizer.step()\n        loss_list.append(loss.data)\n    correct=0\n    for x_test, y_test in validation_loader:\n        # set model to eval \n        model.eval()\n        #make a prediction \n        z = model(x_test)\n        #find max \n        _, yhat = torch.max(z.data, 1)\n       \n        #Calculate misclassified  samples in mini-batch \n        #hint +=(yhat==y_test).sum().item()\n        correct += (yhat==y_test).sum().item()\n   \n    accuracy=correct/N_test"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<b>Print out the Accuracy and plot the loss stored in the list <code>loss_list</code> for every iteration and take a screen shot.</b>"
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "0.9934"
                    },
                    "execution_count": 18,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "accuracy"
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "image/png": "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\n",
                        "text/plain": "<Figure size 432x288 with 1 Axes>"
                    },
                    "metadata": {
                        "needs_background": "light"
                    },
                    "output_type": "display_data"
                }
            ],
            "source": "plt.plot(loss_list)\nplt.xlabel(\"iteration\")\nplt.ylabel(\"loss\")\nplt.show()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<h2 id=\"Question_3\">Question 3: Find the misclassified samples</h2> "
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<b>Identify the first four misclassified samples using the validation data:</b>"
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Sample : 388; Expected Label: tensor([1]); Obtained Label: tensor([0])\nSample : 537; Expected Label: tensor([0]); Obtained Label: tensor([1])\nSample : 788; Expected Label: tensor([1]); Obtained Label: tensor([0])\nSample : 945; Expected Label: tensor([0]); Obtained Label: tensor([1])\n"
                }
            ],
            "source": "count = 0\nmax_num_of_items = 4  # first four mis-classified samples\nvalidation_loader_batch_one = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=1)\n\nfor i, (x_test, y_test) in enumerate(validation_loader_batch_one):\n    # set model to eval\n    model.eval()\n    \n    # make a prediction\n    z = model(x_test)\n    \n    # find max\n    _, yhat = torch.max(z.data, 1)\n    \n    # print mis-classified samples\n    if yhat != y_test:\n        print(\"Sample : {}; Expected Label: {}; Obtained Label: {}\".format(str(i), str(y_test), str(yhat)))\n        count += 1\n        if count >= max_num_of_items:\n            break"
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.6",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.6.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}